{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conditional GANS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discriminator Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the standalone discriminator model\n",
    "def define_discriminator(in_shape=(28,28,1)):\n",
    "  model = Sequential()\n",
    "  # downsample\n",
    "  model.add(Conv2D(128, (3,3), strides=(2,2), padding='same', input_shape=in_shape)) \n",
    "  model.add(LeakyReLU(alpha=0.2))\n",
    "  # downsample\n",
    "  model.add(Conv2D(128, (3,3), strides=(2,2), padding='same')) \n",
    "  model.add(LeakyReLU(alpha=0.2))\n",
    "  # classifier\n",
    "  model.add(Flatten())\n",
    "  model.add(Dropout(0.4))\n",
    "  model.add(Dense(1, activation='sigmoid'))\n",
    "  # compile model\n",
    "  opt = Adam(lr=0.0002, beta_1=0.5)\n",
    "  model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy']) \n",
    "  return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generator Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the standalone generator model\n",
    "def define_generator(latent_dim):\n",
    "  model = Sequential()\n",
    "  # foundation for 7x7 image\n",
    "  n_nodes = 128 * 7 * 7\n",
    "  model.add(Dense(n_nodes, input_dim=latent_dim))\n",
    "  model.add(LeakyReLU(alpha=0.2))\n",
    "  model.add(Reshape((7, 7, 128)))\n",
    "  # upsample to 14x14\n",
    "  model.add(Conv2DTranspose(128, (4,4), strides=(2,2), padding='same')) \n",
    "  model.add(LeakyReLU(alpha=0.2))\n",
    "  # upsample to 28x28\n",
    "  model.add(Conv2DTranspose(128, (4,4), strides=(2,2), padding='same')) \n",
    "  model.add(LeakyReLU(alpha=0.2))\n",
    "  # generate\n",
    "  model.add(Conv2D(1, (7,7), activation='tanh', padding='same')) \n",
    "  return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining both models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the combined generator and discriminator model, for updating the generator\n",
    "def define_gan(generator, discriminator):\n",
    "# make weights in the discriminator not trainable discriminator.trainable = False\n",
    "# connect them\n",
    "model = Sequential()\n",
    "# add generator\n",
    "model.add(generator)\n",
    "# add the discriminator\n",
    "model.add(discriminator)\n",
    "# compile model\n",
    "opt = Adam(lr=0.0002, beta_1=0.5) \n",
    "model.compile(loss='binary_crossentropy', optimizer=opt) \n",
    "return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Latent Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate points in latent space as input for the generator\n",
    "def generate_latent_points(latent_dim, n_samples):\n",
    "  # generate points in the latent space\n",
    "  x_input = randn(latent_dim * n_samples)\n",
    "  # reshape into a batch of inputs for the network\n",
    "  x_input = x_input.reshape(n_samples, latent_dim)\n",
    "  return x_input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Fake Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the generator to generate n fake examples, with class labels\n",
    "def generate_fake_samples(generator, latent_dim, n_samples):\n",
    "  # generate points in latent space\n",
    "  x_input = generate_latent_points(latent_dim, n_samples)\n",
    "  # predict outputs\n",
    "  X = generator.predict(x_input)\n",
    "  # create class labels\n",
    "  y = zeros((n_samples, 1))\n",
    "  return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Real Ramples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select real samples\n",
    "def generate_real_samples(dataset, n_samples):\n",
    "  # choose random instances\n",
    "  ix = randint(0, dataset.shape[0], n_samples)\n",
    "  # select images\n",
    "  X = dataset[ix]\n",
    "  # generate class labels\n",
    "  y = ones((n_samples, 1))\n",
    "  return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Both Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the generator and discriminator\n",
    "def train(g_model, d_model, gan_model, dataset, latent_dim, n_epochs=100, n_batch=128):\n",
    "  bat_per_epo = int(dataset.shape[0] / n_batch)\n",
    "  half_batch = int(n_batch / 2)\n",
    "  # manually enumerate epochs\n",
    "  for i in range(n_epochs):\n",
    "    # enumerate batches over the training set\n",
    "    for j in range(bat_per_epo):\n",
    "      # get randomly selected ✬real✬ samples\n",
    "      X_real, y_real = generate_real_samples(dataset, half_batch)\n",
    "      # update discriminator model weights\n",
    "      d_loss1, _ = d_model.train_on_batch(X_real, y_real)\n",
    "      # generate ✬fake✬ examples\n",
    "      X_fake, y_fake = generate_fake_samples(g_model, latent_dim, half_batch) # update discriminator model weights\n",
    "      d_loss2, _ = d_model.train_on_batch(X_fake, y_fake)\n",
    "      # prepare points in latent space as input for the generator\n",
    "      X_gan = generate_latent_points(latent_dim, n_batch)\n",
    "      # create inverted labels for the fake samples\n",
    "      y_gan = ones((n_batch, 1))\n",
    "      # update the generator via the discriminator✬s error\n",
    "      g_loss = gan_model.train_on_batch(X_gan, y_gan)\n",
    "      # summarize loss on this batch\n",
    "      print('>%d, %d/%d, d1=%.3f, d2=%.3f g=%.3f' %\n",
    "            (i+1, j+1, bat_per_epo, d_loss1, d_loss2, g_loss))\n",
    "      # save the generator model\n",
    "    g_model.save('generator.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# size of the latent space\n",
    "latent_dim = 100\n",
    "# create the discriminator\n",
    "discriminator = define_discriminator()\n",
    "# create the generator\n",
    "generator = define_generator(latent_dim)\n",
    "# create the gan\n",
    "gan_model = define_gan(generator, discriminator)\n",
    "# load image data\n",
    "dataset = load_real_samples()\n",
    "# train model\n",
    "train(generator, discriminator, gan_model, dataset, latent_dim)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machine-learning-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
